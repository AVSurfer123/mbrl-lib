from typing import Optional

import numpy as np

import mbrl.env.reward_fns
import mbrl.models


def evaluate_action_sequences(
    initial_state: np.ndarray,
    action_sequences: np.ndarray,
    model_env: mbrl.models.ModelEnv,
    num_particles: int,
    propagation_method: str,
    reward_fn: Optional[mbrl.env.reward_fns.RewardFnType] = None,
) -> np.ndarray:
    assert len(action_sequences.shape) == 3  # population_size, horizon, action_shape
    population_size, horizon, action_dim = action_sequences.shape
    initial_obs_batch = np.tile(
        initial_state, (num_particles * population_size, 1)
    ).astype(np.float32)
    model_env.reset(initial_obs_batch, propagation_method=propagation_method)

    total_rewards: np.ndarray = 0
    for time_step in range(horizon):
        actions_for_step = action_sequences[:, time_step, :]
        action_batch = np.repeat(actions_for_step, num_particles, axis=0)
        next_obs, pred_rewards, _, _ = model_env.step(action_batch, sample=True)
        rewards = (
            pred_rewards if reward_fn is None else reward_fn(action_batch, next_obs)
        )
        total_rewards += rewards

    return total_rewards


# TODO separate CEM specific parameters. This can probably be a planner class
#   and CEM replaced by some generic optimizer
class CEMPlanner:
    def __init__(
        self,
        num_iterations: int,
        elite_ratio: float,
        population_size: int,
        sigma: float,
    ):
        self.num_iterations = num_iterations
        self.elite_ratio = elite_ratio
        self.population_size = population_size
        self.sigma = sigma

    def plan(
        self,
        model_env: mbrl.models.ModelEnv,
        initial_state: np.ndarray,
        horizon: int,
        num_model_particles: int,
        propagation_method: str,
        reward_fn: Optional[mbrl.env.reward_fns.RewardFnType] = None,
    ) -> np.ndarray:
        def obj_func(action_sequences_: np.ndarray) -> np.ndarray:
            # Returns the mean (over particles) of the total reward for each
            # sequence
            total_rewards = evaluate_action_sequences(
                initial_state,
                action_sequences_,
                model_env,
                num_model_particles,
                propagation_method,
                reward_fn,
            )
            total_rewards = total_rewards.reshape(
                self.population_size, num_model_particles
            )
            return total_rewards.mean(axis=1)

        # -------------------- This section to be generated by CEM
        action_sequences = np.array(
            [
                model_env.action_space.sample()
                for _ in range(self.population_size * horizon)
            ],
            dtype=np.float32,
        )
        action_sequences = action_sequences.reshape((self.population_size, horizon, -1))
        # -------------------- EOS

        return action_sequences
